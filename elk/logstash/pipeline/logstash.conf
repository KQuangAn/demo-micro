input {
	# Beats input for log shippers
	beats {
		port => 5044
	}

	# TCP input for direct log streaming
	tcp {
		port => 50000
		codec => json
	}

	# Kafka input for microservices events
	kafka {
		bootstrap_servers => "kafka:29092"
		topics => ["order.events", "inventory.events", "notification.events"]
		group_id => "logstash-consumer-group"
		codec => json
		consumer_threads => 3
		decorate_events => true
		auto_offset_reset => "earliest"
		# Enable Kafka metadata decoration
		metadata_fields => ["timestamp", "topic", "partition", "offset"]
	}
}

## Filters for log processing and enrichment
filter {
	# Parse JSON messages if they come as strings
	if [message] =~ /^\{.*\}$/ {
		json {
			source => "message"
			remove_field => ["message"]
		}
	}

	# Parse timestamp field to @timestamp
	if [timestamp] {
		date {
			match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd HH:mm:ss"]
			target => "@timestamp"
		}
	}

	# Add service tag from Kafka topic
	if [@metadata][kafka][topic] {
		mutate {
			add_field => {
				"service" => "%{[@metadata][kafka][topic]}"
				"kafka_partition" => "%{[@metadata][kafka][partition]}"
				"kafka_offset" => "%{[@metadata][kafka][offset]}"
			}
		}
	}

	# Extract service name from topic (e.g., order.events -> order)
	if [service] {
		grok {
			match => { "service" => "(?<service_name>[^.]+)\.events" }
			overwrite => ["service_name"]
		}
	}

	# Add environment tag
	mutate {
		add_field => { "environment" => "kubernetes" }
		add_field => { "platform" => "demo-micro" }
	}
}

output {
	# Output to Elasticsearch
	elasticsearch {
		hosts => ["elasticsearch:9200"]
		user => "elastic"
		password => "${ELASTIC_PASSWORD}"
		index => "microservices-%{+YYYY.MM.dd}"
		# Create index template
		manage_template => true
		template_name => "microservices"
		template_overwrite => true
	}

	# Debug output to stdout (can be disabled in production)
	stdout { 
		codec => rubydebug {
			metadata => true
		}
	}
}
